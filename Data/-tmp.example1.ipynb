{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.1.156.138:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = sc.wholeTextFiles('/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/derby.log', \"----------------------------------------------------------------\\nFri Dec 01 15:40:46 PST 2017:\\nBooting Derby version The Apache Software Foundation - Apache Derby - 10.12.1.1 - (1704137): instance a816c00e-0160-1474-2e4e-000007f86f28 \\non database directory /Users/siavashmortezavi/github_repos/2017-msan694-example/Data/metastore_db with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@7e4c1fbb \\nLoaded from file:/usr/local/Cellar/apache-spark/2.2.0/libexec/jars/derby-10.12.1.1.jar\\njava.vendor=Oracle Corporation\\njava.runtime.version=1.8.0_152-b16\\nuser.dir=/Users/siavashmortezavi/github_repos/2017-msan694-example/Data\\nos.name=Mac OS X\\nos.arch=x86_64\\nos.version=10.13.1\\nderby.system.home=null\\nDatabase Class Loader started - derby.database.classpath=''\\n\")\n"
     ]
    }
   ],
   "source": [
    "for file in files.take(1):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/derby.log',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/example.json',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/README.md',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/filtered_registered_business_sf.csv',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/supervisor_sf.csv',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/USF_Mission.txt',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/ignatian_pedagogy',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/-tmp.example1.ipynb',\n",
       " 'file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/walmart_search_san_francisco.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = sc.textFile('~/github_repos/2017-msan694-example/Data/supervisor_sf.csv',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_supervisor = supervisor.filter(lambda x: \"94103,\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o403.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/testfile already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1119)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8003d37fc970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_supervisor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testfile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o403.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/testfile already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1119)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "filtered_supervisor.saveAsTextFile('testfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = sc.textFile('/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/example.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = input.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'id_1', 'array': [1, 2, 3], 'dict': {'key': 'value1'}},\n",
       " {'ID': 'id_2', 'array': [2, 4, 6], 'dict': {'key': 'value2'}},\n",
       " {'ID': 'id_3',\n",
       "  'array': [3, 6, 9],\n",
       "  'dict': {'extra_key': 'extra_value3', 'key': 'value3'}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'id_1', 'array': [1, 2, 3], 'dict': {'key': 'value1'}},\n",
       " {'ID': 'id_3',\n",
       "  'array': [3, 6, 9],\n",
       "  'dict': {'extra_key': 'extra_value3', 'key': 'value3'}}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data_with_3 = data.filter(lambda x: 3 in x['array'])\n",
    "json_data_with_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data_with_3.map(lambda x : json.dumps(x)).saveAsTextFile('json_data_with_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = sc.textFile('/Users/siavashmortezavi/github_repos/2017-msan694-example/Data/filtered_registered_business_sf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = input.map(lambda x: pd.read_pickle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 36, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 94, in read_pickle\n    return try_read(path)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-71-9c6c99e7ee60>\", line 1, in <lambda>\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 97, in read_pickle\n    return try_read(path, encoding='latin1')\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 94, in read_pickle\n    return try_read(path)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-71-9c6c99e7ee60>\", line 1, in <lambda>\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 97, in read_pickle\n    return try_read(path, encoding='latin1')\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-e9821da445d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 36, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 94, in read_pickle\n    return try_read(path)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-71-9c6c99e7ee60>\", line 1, in <lambda>\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 97, in read_pickle\n    return try_read(path, encoding='latin1')\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 94, in read_pickle\n    return try_read(path)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 83, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 88, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-71-9c6c99e7ee60>\", line 1, in <lambda>\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 97, in read_pickle\n    return try_read(path, encoding='latin1')\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 92, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\", line 66, in read_wrapper\n    is_text=False)\n  File \"/Users/siavashmortezavi/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\", line 388, in _get_handle\n    f = open(path_or_buf, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '94123,Tournahu George L,3301 Broderick St,San Francisco,CA'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

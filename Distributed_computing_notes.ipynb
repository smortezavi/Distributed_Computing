{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation**\n",
    "•Use python version 2.7.12. •Install Anaconda\n",
    "•This includes ipython, jupyter •Download JDK\n",
    "brew update\n",
    "      brew cask install java\n",
    "✓Spark runs on Java 8+, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.2.0 uses Scala 2.11. You will need to use a compatible Scala version (2.11.x).\n",
    "https://gist.github.com/ololobus/4c221a0891775eaa86b0\n",
    "\n",
    "Spark Installation Install Spark\n",
    "   brew install apache-spark\n",
    " https://gist.github.com/ololobus/4c221a0891775eaa86b0\n",
    "\n",
    "Spark Installation\n",
    "Spark-shell – uses scala or python.\n",
    "A program written in the shell is discarded\n",
    "after you exit the shell.\n",
    "Testing and developing applications.\n",
    "   spark-shell\n",
    "   pyspark\n",
    "https://gist.github.com/ololobus/4c221a0891775eaa86b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Apache Spark**?\n",
    "\n",
    "Spark adds in-Memory Compute for ETL, Machine Learning and Data Science Workloads to Hadoop.\n",
    "\n",
    "**Explain the key features of Spark**?\n",
    "\n",
    "Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets\n",
    "\n",
    "**What is RDD?**\n",
    "\n",
    "Resilient(fault tolerent) Distributed(distribute the data parallel) Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations.\n",
    "\n",
    "**How to create RDD?**\n",
    "Two ways to create an RDD\n",
    "    1. Loading external data sc.textfile(\"file\")\n",
    "    2. Take a collection such as seq (Array or List) and create RDD and distribute. \n",
    "        lines = sc.parallelize(['spark','spark is fun'])\n",
    "        \n",
    "**Functions of \"spark core\"?**\n",
    "\n",
    "The Bridge or abstraction above Hadoop that contains:\n",
    "    1. Resilient Distributed Dataset (RDD) : Abstraction of a distributed collection of items with operations and transformation applicable to the dataset.\n",
    "    2. Fundamental functions such as networking, security, scheduling and data shuffling.\n",
    "    3. Logic for accessing various filesystems including HDFS, GlusterFS, Amazon S3.\n",
    "\n",
    "**What is \"spark context\"?**\n",
    "letting us know that we are ready to build RDD's\n",
    "    1. Application instance representing the connection to the spark master and workers.\n",
    "Instantiated at the beginning of a Spark\n",
    "\n",
    "    2. application and created by spark driver. Once having a SparkContext, you can use it to build RDDs.\n",
    "\n",
    "**List the various types of \"Cluster managers\" in Spark?**\n",
    "\n",
    "The system currently supports three cluster managers:\n",
    "\n",
    "    1. Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    "    2. Apache Mesos – a general cluster manager that can also run Hadoop MapReduce and service applications.\n",
    "    3. Hadoop YARN – the resource manager in Hadoop 2.\n",
    "    4.Kubernetes (experimental) – In addition to the above, there is experimental support for Kubernetes. Kubernetes is an open-source platform for providing container-centric infrastructure. Kubernetes support is being actively developed in an apache-spark-on-k8s Github organization. For documentation, refer to that project’s README.\n",
    "\n",
    "**What is \"Spark SQL\" (Shark)?**\n",
    "\n",
    "Provides functions for manipulating large sets of\n",
    "distributed, structured data using an SQL subset.\n",
    "    1. Uses DataFrames and DataSets\n",
    "    2. Spark SQL transforms operations on DataFrames/ DataSets to operations on RDDs.\n",
    "    3. Data Sources include Hive, JSON, relational databases, NoSQL databases and Parquet files. \n",
    "\n",
    "**What is \"SparkStreaming\"?**\n",
    "\n",
    "    1. Ingest real-time streaming data from various sources\n",
    "        including HDFS, Kafka, Flume, Twitter, ZeroMQ and custom ones.\n",
    "    2. Recover from failure automatically. Resilient\n",
    "    3. Represent streaming data using discretized streams (Dstreams), which periodically create RDDs containing the data that came in during the last time window.\n",
    "    4. Can be combined with other Spark components (Spark Core, SparkML and Mllib, GraphX, Spark SQL)\n",
    "\n",
    "**What is GraphX?**\n",
    "\n",
    "    1. Provide functions for building graphs, represented as\n",
    "graph RDDs : EdgeRDD and VertexRDD.\n",
    "    2. Contain important algorithms of graph theory such as page rank, connected components, shortest paths, SVD++ \n",
    "\n",
    "**What is \"MLib\"?**\n",
    "\n",
    "Spark MLlib and ML\n",
    "    1. Library of machine learning algorithms\n",
    "    2. Include logistic regression, naïve Bayes, support vector machine, decision trees, random forests, linear regression and k-mean clustering.\n",
    "    3. MLlib : RDD-based APIs.\n",
    "    4. Spark ML : DataFrame-based APIs.\n",
    "\n",
    "**What are the advantages of using Apache Spark over Hadoop MapReduce for big data processing?**\n",
    "\n",
    "Map-reduce:  Map(e.g. filter) Reduce(e.g. count(), sum())\n",
    "\n",
    "Hadoop limitation: Hadoop uses the disk instead of memory. \n",
    "Many problems don’t fit 2 step hadoop process\n",
    "Complexity problems\n",
    "Spark stores intermediate steps on memory instead of on the disk. \n",
    "\n",
    "**What are the languages supported by Apache Spark for developing big data applications?**\n",
    "\n",
    "Java, Scala, Python, R\n",
    "\n",
    "**Can you use Spark to access and analyze data stored in Cassandra databases?**\n",
    "\n",
    "HDFS, Cassandra, HBase, Hive, Tachyon\n",
    "\n",
    "**Name a few companies that use Apache Spark in production?**\n",
    "Yahoo, Amazon, ebay, DATABRICKS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an RDD from loading External Data \n",
    "document = sc.textFile('../Distributed_Computing/Data/README.md',minPartitions=4)\n",
    "\n",
    "#Creating an RDD by passing in a collection of objects in this case a list\n",
    "lines    = sc.parallelize(['1',[1,2,3],(1,2,3)])\n",
    "for stuff in document.collect():  #.collect() creates a list from your RDD object Reduce\n",
    "    print stuff.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.getNumPartitions()     #Return the number of clusters or partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print lines                     #without .collect() an RDD object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print lines.collect()          #Reduce Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is \"Partitions\"?**\n",
    "    A partition in spark is an atomic chunk of data (logical division of data) stored on a node in the cluster. Partitions are basic units of parallelism in Apache\n",
    "\n",
    "**Types or RDD operations**\n",
    "    1. **Transformations**: Produce a new RDD by performing data manipulation on another RDD.\n",
    "       Ex. map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, reduceByKey, aggregateByKey, sortByKey, join, cogroup, cartesian, pipe, coalesce, repartition, repartitionAndSortWithinPartitions.\n",
    "       \n",
    "    2. **Actions**: Trigger a computation to return the result to the calling program or to perform some actions on an RDD’s elements.\n",
    "    Ex. reduce, collect, count, first, take, takeSample, takeOrdered, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile, countByValue, foreach.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x+2       #Example of inline functions\n",
    "f(2)                   #Can be passes as a parameter for RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map(func)**\n",
    "Return a new distributed dataset formed by passing each element of the source through a function func.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = document.map(lambda x:x.split())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter(func)** Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = document.filter(lambda x:'spark' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap** Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = document.flatMap(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapPartions(func)** Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Example needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapPartitionsWithIndex(func)** Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Example needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample(withReplacement, fraction, seed)** Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_sample = y.sample(withReplacement=True, fraction=.25, seed=None)\n",
    "len(y.collect()),len(quarter_sample.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distinct([numTasks]))** Return a new dataset that contains the distinct elements of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**union(otherDataset)** Return a new dataset that contains the union of the elements in the source dataset and the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = sc.parallelize(['1',2,3])\n",
    "print lines1.collect(), len(lines1.collect())\n",
    "print lines.collect(), len(lines.collect())\n",
    "print lines.union(lines1).collect(), len(lines.union(lines1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intersection** Return a new RDD that contains the intersection of elements in the source\n",
    "dataset and the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines2 = sc.parallelize([1,2,'3'])\n",
    "lines1.intersection(lines2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1-1**\n",
    "Load a text file(“ignatian_pedagogy”) and split each line by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sc.textFile(\"../Distributed_Computing/Data/ignatian_pedagogy\")\n",
    "words = text.map(lambda x:x.split())\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1-2**\n",
    "Generate a list of words within one level structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_one_level = text.flatMap(lambda x:x.split())\n",
    "words_one_level.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1-3** \n",
    "Find words including “USF”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_one_level.filter(lambda x:\"USF\" in x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize(range(1,17))\n",
    "numbers.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "Parallelize numbers between 1 and 16. Calculate the count and sum in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ch(items):\n",
    "    new_list = [0,0]\n",
    "    for nums in items:\n",
    "        new_list[0] += 1\n",
    "        new_list[1] += nums\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(iterator): yield sum(iterator) \n",
    "numbers.count(), numbers.sum(), numbers.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.mapPartitions(ch).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1.collect(), lines2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subtract()** •Return elements that are in rdd1 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1.subtract(lines2).collect(), lines2.subtract(lines1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cartesian** Return cartesian product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1.cartesian(lines2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3-1**\n",
    "Find distinct words in “ignatian_pedagogy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignatian = sc.textFile('../Distributed_Computing/Data/ignatian_pedagogy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ignatian.flatMap(lambda x:x.split()).distinct()\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3-2**\n",
    "Create a flatmap of distinct words from “README.md”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = document.flatMap(lambda x:x.split()).distinct()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3-3**\n",
    "What is union, intersection, subtract and cartesian product of the sets from Example 3-1 and Example 3-2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.union(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.intersection(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.subtract(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.subtract(x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Operation Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce(func)**\n",
    "    1. Take a function that operates on two elements of the type in your RDD. \n",
    "    2. Returns a new element of the same type.\n",
    "        Ex. sum = rdd.reduce(lambda x, y = x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "numbers.reduce(lambda x,y:x+y), numbers.fold(1,add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4-1***\n",
    "For the numbers between 1 and 9, calculate sum of the odd numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(range(1,10)).filter(lambda x:(x % 2) == 1).sum(),\\\n",
    "sc.parallelize(range(1,10)).filter(lambda x:(x % 2) == 1).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(range(1,10)).filter(lambda x:(x % 2) == 1).fold(0, lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4-3**\n",
    "Using aggregate(), return (sum, # of elements) of odd numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(range(1,10)).filter(lambda x:(x % 2) == 1).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(range(1,10)).filter(lambda x:(x % 2) == 1).aggregate((0,0),\\\n",
    "lambda x,y:(x[0]+y,x[1]+1),lambda x,y:(x[0]+y[0],x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect()**\n",
    "Return the entire RDD’s contents.\n",
    "\n",
    "**count()**\n",
    "Return the count of elements.\n",
    "\n",
    "**countByValue()**\n",
    "Return the number of times each element occurs in the RDD.\n",
    "\n",
    "**top(n)**\n",
    "Return top elements from an RDD, using the default ordering on the data.\n",
    "\n",
    "**take(n)**\n",
    "Return n elements.\n",
    "\n",
    "**first()**\n",
    "Return the first element of the data.\n",
    "\n",
    "**takeSample(withReplacement, num, seed)**\n",
    "Return a fixed-size sample subset of an RDD.\n",
    "\n",
    "**foreach()**\n",
    "Used for performing computations on each element in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.collect(), numbers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers2 = sc.parallelize([1,1,1,1,2,2,2])\n",
    "numbers2.countByValue(), numbers2.top(5), numbers2.take(6), numbers2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out(x):\n",
    "    print(x)\n",
    "numbers2.takeSample(withReplacement=True,num = 3), numbers2.foreach(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.count(), numbers.sum(), numbers.stdev(), numbers.max(), numbers.min(), numbers.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** to run pyspark in python **\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#Create SparkConf object to configure the application\n",
    "\n",
    "conf = SparkConf().setMaster(“local[*]).setAppName(“AppName”) \n",
    "#Initializing a SparkContext (SC)\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an “RDD lineage”?**\n",
    "\n",
    "\n",
    "**What do you undrestand by Pair RDD?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Creation Pair RDDs**\n",
    "Apply a map function with a lambda or user- defined function to have a pair of (Key, Value).\n",
    "\n",
    "    1.Key : could be a simple object (integer, string, etc.) to complex objects (tuples, etc.).\n",
    "    \n",
    "    2.Value : could be a simple objects to data structures (lists, tuples, dictionaries, sets, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "From the “README.md” file,\n",
    "    1. Extract all the words. (space separated) • Generate key-value pairs of (Word, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words =  document.flatMap(lambda x:x.split()).distinct()\n",
    "key_value_pair = unique_words.map(lambda x: (x,1))\n",
    "key_value_pair.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keys()**\n",
    "Return an RDD of just the keys.\n",
    "\n",
    "**values()**\n",
    "Return an RDD of just the values.\n",
    "\n",
    "\n",
    "**sortByKey()**\n",
    "Return an RDD sorted by the key.\n",
    "\n",
    "**groupByKey()**\n",
    "Group values with the same key.\n",
    "\n",
    "**mapValues(func)**\n",
    "Apply a function to each value of a pair RDD without changing the key.\n",
    "\n",
    "**flatMapValues(func)**\n",
    "\n",
    "Apply a function that returns an iterator to each value of a pair RDD, and for each element returned, produce a key/value entry with the old key. Often used for tokenization.\n",
    "\n",
    "**reduceByKey(func)**\n",
    "Combine values with the same key.\n",
    "\n",
    "**combineByKey(createCombi ner, mergeValue, mergeCombiners)**\n",
    "Combine values with the same key using a different result type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.sortBy(lambda x:x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.mapValues(lambda x:(x,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.flatMapValues(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pair.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "From the “README.md” file,\n",
    "    1. Extract all the words. (space separated)\n",
    "    2. Generate key-value pairs of (Length of Word, Word).\n",
    "    3. Try the following : • keys()\n",
    "    4. values()\n",
    "    5. sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_length = unique_words.map(lambda x:(len(x),x))\n",
    "key_value_length.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_length.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_length.groupByKey().collect()[0][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.flatMap(lambda x:x.split()).\\\n",
    "    map(lambda x:(x,1)).groupByKey().\\\n",
    "        mapValues(lambda x:sum(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.flatMap(lambda x:x.split()).\\\n",
    "    map(lambda x:(len(x),x)).groupByKey().\\\n",
    "        mapValues(lambda x:list(x)).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.flatMap(lambda x:x.split()).\\\n",
    "    map(lambda x:(x,1)).\\\n",
    "    reduceByKey(lambda x,y:x+y).collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1,2,2,2,3,3,5,6]).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CombineByKey**\n",
    "**Example 7**\n",
    "\n",
    "Using combineByKey(), create pairs (Length of words, (Frequency, a list of words)) from “README.md”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = document.flatMap(lambda x:x.split()).map(lambda x:(len(x),x))\n",
    "test.combineByKey((lambda x : (1,x)), (lambda x,y : (x[0]+1, x[1]+\",\"+ y)), \n",
    "                      (lambda x, y : (x[0]+y[0], x[1]+\",\"+y[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which operations don’t shuffle data? \n",
    "\n",
    "mapValues()     doesn't shuffle\n",
    "\n",
    "groupByKey()    shuffles\n",
    "\n",
    "reduceByKey()   shuffles\n",
    "\n",
    "combineBykey()  shuffles\n",
    "\n",
    "sortByKey()     shuffles\n",
    "\n",
    "flatMapValues() doesn't shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sortBy(lambda x:x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3,4])\n",
    "x.getStorageLevel()\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.persist(StorageLevel(False,True,False,True,3)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup = sc.textFile(\"../Distributed_Computing/Data/supervisor_sf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup.map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0],x[1]))\\\n",
    "        .sortBy(lambda x: x[1],ascending =False)\\\n",
    "            .collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sup.map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0],x[1]))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(lambda x:list(x))\\\n",
    "    .sortBy(lambda x:len(x[1]), ascending = False).collect()\n",
    "#print x[0][0]\n",
    "#print x[0][1][0]\n",
    "#print x[0][1][1]\n",
    "print x[0][0]\n",
    "print x[0][1]\n",
    "print len(x[0][1])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is a worker node?**\n",
    "\n",
    "**Spark cluster**\n",
    "\n",
    "A set of interconnected processes, running in a distributed manner on different machines.\n",
    "\n",
    "Types (3)\n",
    "Spark standalone\n",
    "Hadoop YARN\n",
    "Mesos\n",
    "\n",
    "\n",
    "**Spark Runtime Components**\n",
    " 1. Client\n",
    " \n",
    "• Starts the driver program.\n",
    "• Prepares the classpath and all configuration options.\n",
    "• Client process : spark-submit, pyspark, spark-shell scripts\n",
    "\n",
    "2. Driver\n",
    "\n",
    "Orchestrates and monitors executor of an Spark application.\n",
    "Once it gets information from the Spark master of all the workers in the cluster and where they are, the driver program distributes Spark tasks to each worker’s executor.\n",
    "The driver also receives computed results from each executor’s tasks.\n",
    "Always one driver per application.\n",
    "• Executes Spark tasks with a configurable number of cores.\n",
    "• Stores and caches all data partitions in its memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Cluster Components**\n",
    "\n",
    "1. \n",
    "\n",
    "Cluster Manager\n",
    "• Monitors the worker nodes and reserves resources upon request by the master.\n",
    "2. Master\n",
    "• Accepts applications to be run.\n",
    "• Requests resources in the cluster and makes them\n",
    "available to the driver.\n",
    "• Depending on the mode, it acts as a resource\n",
    "manager and decides where and how many executors to launch, and on what Spark workers in the cluster.\n",
    "3. Worker\n",
    "• Upon receiving instructions from Spark Master, the\n",
    "Spark worker JVM launches executors on the worker\n",
    "on behalf of the Spark Driver.\n",
    "• Spark has to be installed on all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format  Structured   Comments\n",
    "\n",
    "Text     File      No One line is one element for a single file.\n",
    "\n",
    "JSON     Semi      Most libraries require one record per line.\n",
    "\n",
    "CSV/TSV    Yes       Contains fixed number of fields per line.\n",
    "\n",
    "Sequence File Yes    Hadoop file format used for key-value data.\n",
    "\n",
    "Object File  Yes     Used for saving data from a Spark job to be consumed by shared code.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "File Formats\n",
    "• Text Format\n",
    "• Load\n",
    "• One line is one element in a single file.\n",
    "• Use textFile(file_name) method.\n",
    "• Can load multiple text files as pair RDDs, with the key\n",
    "being the file name and the value being the file content.\n",
    "• Use wholeTextFiles(dir _name).\n",
    "• Useful when each file represent a certain time period’s\n",
    "data and need to use it for statistics. Ex. sales stat. •\n",
    "\n",
    "Save\n",
    "• Use saveAsTextFile(new_subdir_name) method.\n",
    "• Return multiple output files underneath the\n",
    "new_subdir_name, as Spark writes the output from multiple partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sc.wholeTextFiles(\"../Distributed_Computing/Data/*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/Users/tyler/Desktop/Distributed_Computing/Data/filtered_registered_business_sf.csv\n",
      "file:/Users/tyler/Desktop/Distributed_Computing/Data/supervisor_sf.csv\n"
     ]
    }
   ],
   "source": [
    "for file in files.take(2):\n",
    "    print file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'file:/Users/tyler/Desktop/Distributed_Computing/Data/filtered_registered_business_sf.csv',\n",
       " u'file:/Users/tyler/Desktop/Distributed_Computing/Data/supervisor_sf.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('/Users/tyler/Desktop/Distributed_Computing/Data/supervisor_sf.csv'\n",
    "                    ,minPartitions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lines.filter(lambda x: '94103' in x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.saveAsTextFile('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'94103,8', u'94103,9', u'94103,10', u'94103,6', u'94103,3', u'94103,5'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to input a json file \n",
    "\n",
    "import json\n",
    "\n",
    "data = input.map(lamdbda x: json.loads(x))\n",
    "\n",
    "\n",
    "json_output = data.map(lamdbda x: json.dumps(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = sc.textFile(\"../Distributed_Computing/Data/example.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'{\"ID\":\"id_1\",\"array\":[1,2,3],\"dict\": {\"key\": \"value1\"}}',\n",
       " u'{\"ID\":\"id_2\",\"array\":[2,4,6],\"dict\": {\"key\": \"value2\"}}',\n",
       " u'{\"ID\":\"id_3\",\"array\":[3,6,9],\"dict\": {\"key\": \"value3\", \"extra_key\": \"extra_value3\"}}']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = input.map(lambda x:json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'ID': u'id_1', u'array': [1, 2, 3], u'dict': {u'key': u'value1'}},\n",
       " {u'ID': u'id_3',\n",
       "  u'array': [3, 6, 9],\n",
       "  u'dict': {u'extra_key': u'extra_value3', u'key': u'value3'}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter(lambda x: 3 in x['array']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_output = data.map(lambda x:json.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = sc.textFile(\"../Distributed_Computing/Data/supervisor_sf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvLoader(line):\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input,fieldnames=[\"Zip\",\"Supervisor\"])\n",
    "    return reader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = input.map(csvLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Supervisor': '8', 'Zip': '94102'},\n",
       " {'Supervisor': '6', 'Zip': '94102'},\n",
       " {'Supervisor': '3', 'Zip': '94102'},\n",
       " {'Supervisor': '5', 'Zip': '94102'},\n",
       " {'Supervisor': '8', 'Zip': '94103'},\n",
       " {'Supervisor': '9', 'Zip': '94103'},\n",
       " {'Supervisor': '10', 'Zip': '94103'},\n",
       " {'Supervisor': '6', 'Zip': '94103'},\n",
       " {'Supervisor': '3', 'Zip': '94103'},\n",
       " {'Supervisor': '5', 'Zip': '94103'},\n",
       " {'Supervisor': '6', 'Zip': '94104'},\n",
       " {'Supervisor': '3', 'Zip': '94104'},\n",
       " {'Supervisor': '6', 'Zip': '94105'},\n",
       " {'Supervisor': '3', 'Zip': '94105'},\n",
       " {'Supervisor': '10', 'Zip': '94107'},\n",
       " {'Supervisor': '6', 'Zip': '94107'},\n",
       " {'Supervisor': '6', 'Zip': '94108'},\n",
       " {'Supervisor': '3', 'Zip': '94108'},\n",
       " {'Supervisor': '2', 'Zip': '94109'},\n",
       " {'Supervisor': '6', 'Zip': '94109'},\n",
       " {'Supervisor': '3', 'Zip': '94109'},\n",
       " {'Supervisor': '5', 'Zip': '94109'},\n",
       " {'Supervisor': '8', 'Zip': '94110'},\n",
       " {'Supervisor': '11', 'Zip': '94110'},\n",
       " {'Supervisor': '9', 'Zip': '94110'},\n",
       " {'Supervisor': '10', 'Zip': '94110'},\n",
       " {'Supervisor': '6', 'Zip': '94111'},\n",
       " {'Supervisor': '3', 'Zip': '94111'},\n",
       " {'Supervisor': '7', 'Zip': '94112'},\n",
       " {'Supervisor': '8', 'Zip': '94112'},\n",
       " {'Supervisor': '11', 'Zip': '94112'},\n",
       " {'Supervisor': '9', 'Zip': '94112'},\n",
       " {'Supervisor': '10', 'Zip': '94112'},\n",
       " {'Supervisor': '7', 'Zip': '94114'},\n",
       " {'Supervisor': '8', 'Zip': '94114'},\n",
       " {'Supervisor': '5', 'Zip': '94114'},\n",
       " {'Supervisor': '2', 'Zip': '94115'},\n",
       " {'Supervisor': '1', 'Zip': '94115'},\n",
       " {'Supervisor': '5', 'Zip': '94115'},\n",
       " {'Supervisor': '7', 'Zip': '94116'},\n",
       " {'Supervisor': '4', 'Zip': '94116'},\n",
       " {'Supervisor': '1', 'Zip': '94117'},\n",
       " {'Supervisor': '7', 'Zip': '94117'},\n",
       " {'Supervisor': '8', 'Zip': '94117'},\n",
       " {'Supervisor': '5', 'Zip': '94117'},\n",
       " {'Supervisor': '2', 'Zip': '94118'},\n",
       " {'Supervisor': '1', 'Zip': '94118'},\n",
       " {'Supervisor': '5', 'Zip': '94118'},\n",
       " {'Supervisor': '2', 'Zip': '94121'},\n",
       " {'Supervisor': '1', 'Zip': '94121'},\n",
       " {'Supervisor': '1', 'Zip': '94122'},\n",
       " {'Supervisor': '7', 'Zip': '94122'},\n",
       " {'Supervisor': '5', 'Zip': '94122'},\n",
       " {'Supervisor': '4', 'Zip': '94122'},\n",
       " {'Supervisor': '2', 'Zip': '94123'},\n",
       " {'Supervisor': '9', 'Zip': '94124'},\n",
       " {'Supervisor': '10', 'Zip': '94124'},\n",
       " {'Supervisor': '7', 'Zip': '94127'},\n",
       " {'Supervisor': '8', 'Zip': '94127'},\n",
       " {'Supervisor': '11', 'Zip': '94127'},\n",
       " {'Supervisor': '2', 'Zip': '94129'},\n",
       " {'Supervisor': '6', 'Zip': '94130'},\n",
       " {'Supervisor': '7', 'Zip': '94131'},\n",
       " {'Supervisor': '8', 'Zip': '94131'},\n",
       " {'Supervisor': '5', 'Zip': '94131'},\n",
       " {'Supervisor': '7', 'Zip': '94132'},\n",
       " {'Supervisor': '11', 'Zip': '94132'},\n",
       " {'Supervisor': '4', 'Zip': '94132'},\n",
       " {'Supervisor': '2', 'Zip': '94133'},\n",
       " {'Supervisor': '3', 'Zip': '94133'},\n",
       " {'Supervisor': '11', 'Zip': '94134'},\n",
       " {'Supervisor': '9', 'Zip': '94134'},\n",
       " {'Supervisor': '10', 'Zip': '94134'},\n",
       " {'Supervisor': '10', 'Zip': '94158'},\n",
       " {'Supervisor': '6', 'Zip': '94158'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
